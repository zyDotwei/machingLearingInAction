{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = 3 * np.random.random(size=100)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x * 3. + 4. + np.random.normal(0, 1, size=100)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用梯度下降法训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(theta, x_b, y):\n",
    "    try:\n",
    "        return np.sum((y - x_b.dot(theta)) ** 2) / len(y)\n",
    "    except:\n",
    "        return float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DJ(theta, x_b, y):\n",
    "    res = np.empty(len(theta))\n",
    "    \n",
    "    res[0] = np.sum(x_b.dot(theta) - y)\n",
    "    \n",
    "    for i in range(1, len(theta)):\n",
    "        res[i] = (x_b.dot(theta) - y).dot(x_b[:, i])\n",
    "    \n",
    "    return res * 2 / len(x_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x_b, y, int_theta, eta, n_iters=1e4, epsilon=1e-8):\n",
    "    theta = int_theta\n",
    "    i_ters = 0\n",
    "    \n",
    "    while i_ters < n_iters:\n",
    "        gradient = DJ(theta, x_b, y)\n",
    "        last_theta = theta\n",
    "        theta = theta - eta * gradient\n",
    "        \n",
    "        if (abs(J(theta, x_b, y) - J(last_theta, x_b, y)) < epsilon):\n",
    "            break\n",
    "        i_ters += 1\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b = np.hstack([np.ones((len(X), 1)), X]) # 为x添加一列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ini_theta = np.zeros(X_b.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = gradient_descent(X_b, y,ini_theta, eta)\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_LinearRegnession:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.conf_ = None\n",
    "        self.itercept_ = None\n",
    "        self._theta = None\n",
    "        \n",
    "    def fit_normal(self, x_train, y_train):\n",
    "        assert x_train.shape[0] == y_train.shape[0], \\\n",
    "            \"the size\"\n",
    "        X_b = np.hstack([np.ones((len(x_train), 1)),x_train])\n",
    "        self._theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_train)  # np.linalg.inv 求逆\n",
    "        self.itercept_ = self._theta[0]\n",
    "        self.conf_ = self._theta[1:]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def fit_gb(self, x_train, y_train):\n",
    "        assert x_train.shape[0] == y_train.shape[0], \\\n",
    "            \"the size\"\n",
    "        def J(theta, x_b, y):\n",
    "            try:\n",
    "                return np.sum((y - x_b.dot(theta)) ** 2) / len(y)\n",
    "            except:\n",
    "                return float('inf')\n",
    "        \n",
    "        def DJ(theta, x_b, y):\n",
    "            res = np.empty(len(theta))\n",
    "\n",
    "            res[0] = np.sum(x_b.dot(theta) - y)\n",
    "\n",
    "            for i in range(1, len(theta)):\n",
    "                res[i] = (x_b.dot(theta) - y).dot(x_b[:, i])\n",
    "\n",
    "            return res * 2 / len(x_b)\n",
    "        \n",
    "        def gradient_descent(x_b, y, int_theta, eta, n_iters=1e4, epsilon=1e-8):\n",
    "            theta = int_theta\n",
    "            i_ters = 0\n",
    "\n",
    "            while i_ters < n_iters:\n",
    "                gradient = DJ(theta, x_b, y)\n",
    "                last_theta = theta\n",
    "                theta = theta - eta * gradient\n",
    "\n",
    "                if (abs(J(theta, x_b, y) - J(last_theta, x_b, y)) < epsilon):\n",
    "                    break\n",
    "                i_ters += 1\n",
    "\n",
    "            return theta\n",
    "        \n",
    "        X_b = np.hstack([np.ones((len(X), 1)), X]) # 为x添加一列\n",
    "        ini_theta = np.zeros(X_b.shape[1])\n",
    "        self._theta = gradient_descent(X_b, y,ini_theta, eta)\n",
    "        self.itercept_ = self._theta[0]\n",
    "        self.conf_ = self._theta[1:]\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def predict(self, x_predict):\n",
    "        assert self.itercept_ is not None and self.conf_ is not None, \\\n",
    "            \"fit before\"\n",
    "        assert x_predict.shape[1] == len(self.conf_), \\\n",
    "            \"number \"\n",
    "        X_b = np.hstack([np.ones((len(x_predict), 1)), x_predict])\n",
    "        \n",
    "        return X_b.dot(self._theta)\n",
    "    \n",
    "    def my_mean_squared_error(self, y_true, y_predict):\n",
    "        return np.sum((y_predict - y_true) ** 2) / len(y_true)\n",
    "    \n",
    "    def my_r2_score(self, y_true, y_predict):\n",
    "        return 1 - self.my_mean_squared_error(y_true, y_predict) / np.var(y_true)\n",
    " \n",
    "    def score(self, x_test, y_test):\n",
    "        y_redict = self.predict(x_test)\n",
    "        return self.my_r2_score(y_test, y_redict)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"my_LinearRegnession()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = my_LinearRegnession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.fit_gb(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.conf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.itercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度下降法的向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_LinearRegnession:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.conf_ = None\n",
    "        self.itercept_ = None\n",
    "        self._theta = None\n",
    "        \n",
    "    def fit_normal(self, x_train, y_train):\n",
    "        assert x_train.shape[0] == y_train.shape[0], \\\n",
    "            \"the size\"\n",
    "        X_b = np.hstack([np.ones((len(x_train), 1)),x_train])\n",
    "        self._theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_train)  # np.linalg.inv 求逆\n",
    "        self.itercept_ = self._theta[0]\n",
    "        self.conf_ = self._theta[1:]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def fit_gb(self, x_train, y_train, eta=0.01, n_iters=1e4):\n",
    "        assert x_train.shape[0] == y_train.shape[0], \\\n",
    "            \"the size\"\n",
    "        def J(theta, x_b, y):\n",
    "            try:\n",
    "                return np.sum((y - x_b.dot(theta)) ** 2) / len(y)\n",
    "            except:\n",
    "                return float('inf')\n",
    "        \n",
    "        def DJ(theta, x_b, y):\n",
    "#             res = np.empty(len(theta))\n",
    "\n",
    "#             res[0] = np.sum(x_b.dot(theta) - y)\n",
    "\n",
    "#             for i in range(1, len(theta)):\n",
    "#                 res[i] = (x_b.dot(theta) - y).dot(x_b[:, i])\n",
    "\n",
    "#             return res * 2 / len(x_b)\n",
    "            return x_b.T.dot(x_b.dot(theta) - y) * 2 / len(y)\n",
    "        \n",
    "        def gradient_descent(x_b, y, int_theta, eta, n_iters=1e4, epsilon=1e-8):\n",
    "            theta = int_theta\n",
    "            i_ters = 0\n",
    "\n",
    "            while i_ters < n_iters:\n",
    "                gradient = DJ(theta, x_b, y)\n",
    "                last_theta = theta\n",
    "                theta = theta - eta * gradient\n",
    "\n",
    "                if (abs(J(theta, x_b, y) - J(last_theta, x_b, y)) < epsilon):\n",
    "                    break\n",
    "                i_ters += 1\n",
    "\n",
    "            return theta\n",
    "        \n",
    "        X_b = np.hstack([np.ones((len(x_train), 1)), x_train]) # 为x添加一列\n",
    "        ini_theta = np.zeros(X_b.shape[1])\n",
    "        self._theta = gradient_descent(X_b, y_train,ini_theta, eta,n_iters)\n",
    "        self.itercept_ = self._theta[0]\n",
    "        self.conf_ = self._theta[1:]\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def predict(self, x_predict):\n",
    "        assert self.itercept_ is not None and self.conf_ is not None, \\\n",
    "            \"fit before\"\n",
    "        assert x_predict.shape[1] == len(self.conf_), \\\n",
    "            \"number \"\n",
    "        X_b = np.hstack([np.ones((len(x_predict), 1)), x_predict])\n",
    "        \n",
    "        return X_b.dot(self._theta)\n",
    "    \n",
    "    def my_mean_squared_error(self, y_true, y_predict):\n",
    "        return np.sum((y_predict - y_true) ** 2) / len(y_true)\n",
    "    \n",
    "    def my_r2_score(self, y_true, y_predict):\n",
    "        return 1 - self.my_mean_squared_error(y_true, y_predict) / np.var(y_true)\n",
    " \n",
    "    def score(self, x_test, y_test):\n",
    "        y_redict = self.predict(x_test)\n",
    "        return self.my_r2_score(y_test, y_redict)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"my_LinearRegnession()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = datasets.load_boston()\n",
    "\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "X = X[y < 50.0]\n",
    "y = y[y < 50.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用最小二乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_req1 = my_LinearRegnession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 92.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "my_LinearRegnession()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time lin_req1.fit_normal(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8129794056212793"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_req1.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用梯度下降法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_req2 = my_LinearRegnession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\wzy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: RuntimeWarning: overflow encountered in square\n",
      "d:\\Users\\wzy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:47: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.17 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "my_LinearRegnession()"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time lin_req2.fit_gb(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_req2.conf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.42362e+01, 0.00000e+00, 1.81000e+01, 0.00000e+00, 6.93000e-01,\n",
       "        6.34300e+00, 1.00000e+02, 1.57410e+00, 2.40000e+01, 6.66000e+02,\n",
       "        2.02000e+01, 3.96900e+02, 2.03200e+01],\n",
       "       [3.67822e+00, 0.00000e+00, 1.81000e+01, 0.00000e+00, 7.70000e-01,\n",
       "        5.36200e+00, 9.62000e+01, 2.10360e+00, 2.40000e+01, 6.66000e+02,\n",
       "        2.02000e+01, 3.80790e+02, 1.01900e+01],\n",
       "       [1.04690e-01, 4.00000e+01, 6.41000e+00, 1.00000e+00, 4.47000e-01,\n",
       "        7.26700e+00, 4.90000e+01, 4.78720e+00, 4.00000e+00, 2.54000e+02,\n",
       "        1.76000e+01, 3.89250e+02, 6.05000e+00],\n",
       "       [1.15172e+00, 0.00000e+00, 8.14000e+00, 0.00000e+00, 5.38000e-01,\n",
       "        5.70100e+00, 9.50000e+01, 3.78720e+00, 4.00000e+00, 3.07000e+02,\n",
       "        2.10000e+01, 3.58770e+02, 1.83500e+01],\n",
       "       [6.58800e-02, 0.00000e+00, 2.46000e+00, 0.00000e+00, 4.88000e-01,\n",
       "        7.76500e+00, 8.33000e+01, 2.74100e+00, 3.00000e+00, 1.93000e+02,\n",
       "        1.78000e+01, 3.95560e+02, 7.56000e+00],\n",
       "       [2.49800e-02, 0.00000e+00, 1.89000e+00, 0.00000e+00, 5.18000e-01,\n",
       "        6.54000e+00, 5.97000e+01, 6.26690e+00, 1.00000e+00, 4.22000e+02,\n",
       "        1.59000e+01, 3.89960e+02, 8.65000e+00],\n",
       "       [7.75223e+00, 0.00000e+00, 1.81000e+01, 0.00000e+00, 7.13000e-01,\n",
       "        6.30100e+00, 8.37000e+01, 2.78310e+00, 2.40000e+01, 6.66000e+02,\n",
       "        2.02000e+01, 2.72210e+02, 1.62300e+01],\n",
       "       [9.88430e-01, 0.00000e+00, 8.14000e+00, 0.00000e+00, 5.38000e-01,\n",
       "        5.81300e+00, 1.00000e+02, 4.09520e+00, 4.00000e+00, 3.07000e+02,\n",
       "        2.10000e+01, 3.94540e+02, 1.98800e+01],\n",
       "       [1.14320e-01, 0.00000e+00, 8.56000e+00, 0.00000e+00, 5.20000e-01,\n",
       "        6.78100e+00, 7.13000e+01, 2.85610e+00, 5.00000e+00, 3.84000e+02,\n",
       "        2.09000e+01, 3.95580e+02, 7.67000e+00],\n",
       "       [5.69175e+00, 0.00000e+00, 1.81000e+01, 0.00000e+00, 5.83000e-01,\n",
       "        6.11400e+00, 7.98000e+01, 3.54590e+00, 2.40000e+01, 6.66000e+02,\n",
       "        2.02000e+01, 3.92680e+02, 1.49800e+01]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.21 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "my_LinearRegnession()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time lin_req2.fit_gb(x_train, y_train, eta=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.09221871,  0.11615068, -0.07233536,  0.00294593,  0.00443085,\n",
       "        0.12400232,  0.05400738,  0.0408687 , -0.00472105,  0.00279527,\n",
       "        0.13037331,  0.04429628, -0.23062717])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_req2.conf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2860060019209706"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_req2.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 54s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "my_LinearRegnession()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time lin_req2.fit_gb(x_train, y_train, eta=0.000001, n_iters=1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.761600988173621"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_req2.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.19626758e-02,  5.43718225e-02, -7.17672640e-02,  1.94538001e-01,\n",
       "        2.01314205e-01,  3.99553907e+00,  3.08389538e-03, -4.99889297e-01,\n",
       "        1.14340764e-01, -9.74944922e-03,  1.85122690e-02,  1.57661792e-02,\n",
       "       -3.73563407e-01])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_req2.conf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据归一化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "standarScaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standarScaler.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_stand = standarScaler.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg3 = my_LinearRegnession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 636 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "my_LinearRegnession()"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time lin_reg3.fit_gb(x_train_stand, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.04042202,  0.83093351, -0.24794356,  0.01179456, -1.35034756,\n",
       "        2.25074   , -0.66384353, -2.53568774,  2.25572406, -2.34011572,\n",
       "       -1.76565394,  0.70923397, -2.72677064])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg3.conf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.22372711, -0.47293831,  1.0044968 , -0.23791548,  1.17672219,\n",
       "         0.1746197 ,  1.12041788, -1.0461857 ,  1.66456428,  1.53040651,\n",
       "         0.77931907,  0.43942982,  1.01121088],\n",
       "       [ 0.00285336, -0.47293831,  1.0044968 , -0.23791548,  1.83672499,\n",
       "        -1.31943651,  0.98737038, -0.79665733,  1.66456428,  1.53040651,\n",
       "         0.77931907,  0.26215642, -0.40385575]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_stand[:2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 14.2362 ,   0.     ,  18.1    ,   0.     ,   0.693  ,   6.343  ,\n",
       "        100.     ,   1.5741 ,  24.     , 666.     ,  20.2    , 396.9    ,\n",
       "         20.32   ],\n",
       "       [  3.67822,   0.     ,  18.1    ,   0.     ,   0.77   ,   5.362  ,\n",
       "         96.2    ,   2.1036 ,  24.     , 666.     ,  20.2    , 380.79   ,\n",
       "         10.19   ]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_stand = standarScaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8129873310487505"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg3.score(x_test_stand, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
